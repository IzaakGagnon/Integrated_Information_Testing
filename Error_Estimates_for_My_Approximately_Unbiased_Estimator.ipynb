{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7t2HWgg99jdA1Fao+x+LR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IzaakGagnon/Integrated_Information_Testing/blob/main/Error_Estimates_for_My_Approximately_Unbiased_Estimator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"git+https://github.com/wmayner/pyphi.git@feature/update-packaging#egg=pyphi[parallel]\"\n",
        "\n",
        "import numpy as np\n",
        "import pyphi\n",
        "import random\n",
        "import time\n",
        "pyphi.config.PROGRESS_BARS = False\n",
        "pyphi.config.SHORTCIRCUIT_SIA = True\n",
        "pyphi.config.WELCOME_OFF = True\n",
        "pyphi.config.REPERTOIRE_DISTANCE = \"GENERALIZED_INTRINSIC_DIFFERENCE\"\n",
        "pyphi.config.PARALLEL = False\n",
        "from scipy.stats import t\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from scipy.integrate import cumtrapz\n",
        "from scipy.integrate import quad\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.stats.sampling import NumericalInverseHermite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2Jbr3NM4wOg",
        "outputId": "a4961608-8cd7-43b5-f8de-f4cc259ece7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: git+https://github.com/wmayner/pyphi.git@feature/update-packaging#egg=pyphi[parallel] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pyphi[parallel]\n",
            "  Cloning https://github.com/wmayner/pyphi.git (to revision feature/update-packaging) to /tmp/pip-install-8uawledu/pyphi_7396e0d7479547f890364e072fd0f35b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/wmayner/pyphi.git /tmp/pip-install-8uawledu/pyphi_7396e0d7479547f890364e072fd0f35b\n",
            "  Running command git checkout -b feature/update-packaging --track origin/feature/update-packaging\n",
            "  Switched to a new branch 'feature/update-packaging'\n",
            "  Branch 'feature/update-packaging' set up to track remote branch 'feature/update-packaging' from 'origin'.\n",
            "  Resolved https://github.com/wmayner/pyphi.git to commit 7a4e23808ddffa00cd393257a830ab0b62310129\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Graphillion>=1.5 (from pyphi[parallel])\n",
            "  Downloading Graphillion-1.8.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting igraph>=0.9.10 (from pyphi[parallel])\n",
            "  Downloading igraph-0.11.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (1.4.2)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (10.1.0)\n",
            "Requirement already satisfied: networkx>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (3.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (1.25.2)\n",
            "Collecting ordered-set>=4.0.2 (from pyphi[parallel])\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (2.0.3)\n",
            "Requirement already satisfied: plotly>=5.8.2 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (5.15.0)\n",
            "Requirement already satisfied: psutil>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (5.9.5)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (6.0.1)\n",
            "Collecting redis>=2.10.5 (from pyphi[parallel])\n",
            "  Downloading redis-5.0.7-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.1/252.1 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (1.11.4)\n",
            "Requirement already satisfied: tblib>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (3.0.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.20.0 in /usr/local/lib/python3.10/dist-packages (from pyphi[parallel]) (4.66.4)\n",
            "Collecting ray[default]>=1.9.2 (from pyphi[parallel])\n",
            "  Downloading ray-2.32.0-cp310-cp310-manylinux2014_x86_64.whl (65.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from Graphillion>=1.5->pyphi[parallel]) (0.18.3)\n",
            "Collecting texttable>=1.6.2 (from igraph>=0.9.10->pyphi[parallel])\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyphi[parallel]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyphi[parallel]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyphi[parallel]) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.8.2->pyphi[parallel]) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.8.2->pyphi[parallel]) (24.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (3.15.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (1.0.8)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (3.20.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (2.31.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (3.9.5)\n",
            "Collecting aiohttp-cors (from ray[default]>=1.9.2->pyphi[parallel])\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful (from ray[default]>=1.9.2->pyphi[parallel])\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-spy>=0.2.0 (from ray[default]>=1.9.2->pyphi[parallel])\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencensus (from ray[default]>=1.9.2->pyphi[parallel])\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (2.8.2)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (0.20.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (7.0.4)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default]>=1.9.2->pyphi[parallel])\n",
            "  Downloading virtualenv-20.26.3-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]>=1.9.2->pyphi[parallel]) (1.64.1)\n",
            "Collecting memray (from ray[default]>=1.9.2->pyphi[parallel])\n",
            "  Downloading memray-1.13.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis>=2.10.5->pyphi[parallel]) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]>=1.9.2->pyphi[parallel]) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]>=1.9.2->pyphi[parallel]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]>=1.9.2->pyphi[parallel]) (1.9.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]>=1.9.2->pyphi[parallel]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]>=1.9.2->pyphi[parallel]) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]>=1.9.2->pyphi[parallel]) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyphi[parallel]) (1.16.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=1.9.2->pyphi[parallel])\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]>=1.9.2->pyphi[parallel]) (4.2.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]>=1.9.2->pyphi[parallel]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]>=1.9.2->pyphi[parallel]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]>=1.9.2->pyphi[parallel]) (0.19.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from memray->ray[default]>=1.9.2->pyphi[parallel]) (3.1.4)\n",
            "Requirement already satisfied: rich>=11.2.0 in /usr/local/lib/python3.10/dist-packages (from memray->ray[default]>=1.9.2->pyphi[parallel]) (13.7.1)\n",
            "Collecting textual>=0.41.0 (from memray->ray[default]>=1.9.2->pyphi[parallel])\n",
            "  Downloading textual-0.73.0-py3-none-any.whl (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.4/564.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencensus-context>=0.1.3 (from opencensus->ray[default]>=1.9.2->pyphi[parallel])\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default]>=1.9.2->pyphi[parallel]) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]>=1.9.2->pyphi[parallel]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]>=1.9.2->pyphi[parallel]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]>=1.9.2->pyphi[parallel]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]>=1.9.2->pyphi[parallel]) (2024.7.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open->ray[default]>=1.9.2->pyphi[parallel]) (1.14.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=1.9.2->pyphi[parallel]) (1.63.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=1.9.2->pyphi[parallel]) (1.24.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=1.9.2->pyphi[parallel]) (2.27.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.9->memray->ray[default]>=1.9.2->pyphi[parallel]) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.2.0->memray->ray[default]>=1.9.2->pyphi[parallel]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.2.0->memray->ray[default]>=1.9.2->pyphi[parallel]) (2.16.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=1.9.2->pyphi[parallel]) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=1.9.2->pyphi[parallel]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=1.9.2->pyphi[parallel]) (4.9)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[default]>=1.9.2->pyphi[parallel]) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[default]>=1.9.2->pyphi[parallel]) (2.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[default]>=1.9.2->pyphi[parallel]) (0.4.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[default]>=1.9.2->pyphi[parallel]) (1.0.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]>=1.9.2->pyphi[parallel]) (0.6.0)\n",
            "Building wheels for collected packages: Graphillion, pyphi\n",
            "  Building wheel for Graphillion (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Graphillion: filename=Graphillion-1.8-cp310-cp310-linux_x86_64.whl size=6957516 sha256=ce5d9c367b0e02c34bbe2cb1db2d6f30d3b325750021372c9b8df9af81c11feb\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/d4/380d0c8b37ccd22ec2b4ecc0b3bcc2731180514af59d8f3623\n",
            "  Building wheel for pyphi (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyphi: filename=pyphi-2.0.0a1-py3-none-any.whl size=706925 sha256=d9bba4d5fe223ddd11e0e38f6a7d24ffea3f2f78ef1281aecc74d925759100da\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mub5h4qo/wheels/ad/82/67/e72d69cdf0abe645447e75f1b0889c490970f1460468a7bc50\n",
            "Successfully built Graphillion pyphi\n",
            "Installing collected packages: texttable, py-spy, opencensus-context, distlib, colorful, virtualenv, redis, ordered-set, igraph, Graphillion, pyphi, aiohttp-cors, textual, ray, opencensus, memray\n",
            "Successfully installed Graphillion-1.8 aiohttp-cors-0.7.0 colorful-0.5.6 distlib-0.3.8 igraph-0.11.6 memray-1.13.4 opencensus-0.11.4 opencensus-context-0.1.3 ordered-set-4.1.0 py-spy-0.3.14 pyphi-2.0.0a1 ray-2.32.0 redis-5.0.7 texttable-1.7.0 textual-0.73.0 virtualenv-20.26.3\n",
            "\n",
            "Welcome to PyPhi!\n",
            "\n",
            "If you use PyPhi in your research, please cite the paper:\n",
            "\n",
            "  Mayner WGP, Marshall W, Albantakis L, Findlay G, Marchman R, Tononi G.\n",
            "  (2018). PyPhi: A toolbox for integrated information theory.\n",
            "  PLOS Computational Biology 14(7): e1006343.\n",
            "  https://doi.org/10.1371/journal.pcbi.1006343\n",
            "\n",
            "Documentation is available online (or with the built-in `help()` function):\n",
            "  https://pyphi.readthedocs.io\n",
            "\n",
            "To report issues, please use the issue tracker on the GitHub repository:\n",
            "  https://github.com/wmayner/pyphi\n",
            "\n",
            "For general discussion, you are welcome to join the pyphi-users group:\n",
            "  https://groups.google.com/forum/#!forum/pyphi-users\n",
            "\n",
            "To suppress this message, either:\n",
            "  - Set `WELCOME_OFF: true` in your `pyphi_config.yml` file, or\n",
            "  - Set the environment variable PYPHI_WELCOME_OFF to any value in your shell:\n",
            "        export PYPHI_WELCOME_OFF='yes'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C-j959ZX4ujo"
      },
      "outputs": [],
      "source": [
        "### KDE Work:\n",
        "\n",
        "\n",
        "from abc import get_cache_token\n",
        "from pyphi.new_big_phi import *\n",
        "import matplotlib.pyplot as plt\n",
        "import functools\n",
        "import itertools\n",
        "from itertools import chain, product\n",
        "import numpy as np\n",
        "from more_itertools import distinct_permutations\n",
        "from toolz import unique\n",
        "from pyphi import combinatorics\n",
        "from pyphi.cache import cache\n",
        "from pyphi.conf import config, fallback\n",
        "from pyphi.direction import Direction\n",
        "from pyphi.models.cuts import (\n",
        "    Bipartition,\n",
        "    CompleteGeneralKCut,\n",
        "    CompleteGeneralSetPartition,\n",
        "    Cut,\n",
        "    GeneralKCut,\n",
        "    GeneralSetPartition,\n",
        "    KPartition,\n",
        "    Part,\n",
        "    SystemPartition,\n",
        "    Tripartition,)\n",
        "from pyphi.partition import system_partition_types\n",
        "from pyphi.registry import Registry\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@system_partition_types.register(\"K-PARTITIONS\")\n",
        "def _unidirectional_set_partitions2(node_indices, node_labels=None):\n",
        "    \"\"\"Generate all unidirectional set partitions of a set of nodes.\"\"\"\n",
        "\n",
        "\n",
        "    ### The Trivial Cut: No Partitions Made\n",
        "    if len(node_indices) == 1 or config.SYSTEM_PARTITION_INCLUDE_COMPLETE:\n",
        "        yield CompleteGeneralSetPartition(node_indices, node_labels=node_labels)\n",
        "\n",
        "\n",
        "\n",
        "    _node_indices = set(range(len(node_indices)))\n",
        "\n",
        "    for partition in combinatorics.set_partitions(_node_indices, nontrivial=True):\n",
        "        for directions in product(Direction.all(), repeat=len(partition)):\n",
        "            if Direction.CAUSE in directions and Direction.EFFECT in directions:\n",
        "\n",
        "                ### Fill out cut Matrix for a partition-direction grouping\n",
        "              cut_matrix = np.zeros([len(_node_indices), len(_node_indices)], dtype=int)\n",
        "              for part, direction in zip(partition, directions):\n",
        "                  nonpart = list(_node_indices - set(part))\n",
        "                  if direction == Direction.CAUSE:\n",
        "                      source, target = nonpart, part\n",
        "                  else:\n",
        "                      source, target = part, nonpart\n",
        "                  cut_matrix[np.ix_(source, target)] = 1\n",
        "                  if direction == Direction.BIDIRECTIONAL:\n",
        "                      cut_matrix[np.ix_(target, source)] = 1\n",
        "\n",
        "\n",
        "               ### Yield all the cuts\n",
        "              yield GeneralSetPartition(\n",
        "                  node_indices,\n",
        "                  cut_matrix,\n",
        "                  node_labels=node_labels,\n",
        "                  set_partition=partition)\n",
        "\n",
        "              ### Yield the cut ignored by the if (both directions in set)\n",
        "    yield GeneralSetPartition(\n",
        "                node_indices,\n",
        "                np.ones(     (len(_node_indices), len(_node_indices))) - np.identity(len(_node_indices)),\n",
        "                node_labels=node_labels,\n",
        "                set_partition=partition,\n",
        "            )\n",
        "\n",
        "### Full Implementation Of The Faster Bi-Partition Algorithm\n",
        "@system_partition_types.register(\"BI-PARTITIONS\")\n",
        "def generate_all_bipartitions(node_indices, node_labels=None):\n",
        "\n",
        "\n",
        "  ### Perform the trivial cut\n",
        "  if len(node_indices) == 1 or config.SYSTEM_PARTITION_INCLUDE_COMPLETE:\n",
        "        yield CompleteGeneralSetPartition(node_indices, node_labels=node_labels)\n",
        "\n",
        "  _node_indices = set(range(len(node_indices)))\n",
        "\n",
        "\n",
        "  ### Loop through the possible partitions of the network.\n",
        "  for partition in get_bipartitions(_node_indices):\n",
        "      for directions in product(Direction.all(), repeat=len(partition)):\n",
        "            if Direction.CAUSE in directions and Direction.EFFECT in directions:\n",
        "\n",
        "              ### Fill out cut Matrix for a partition-direction grouping\n",
        "              cut_matrix = np.zeros([len(_node_indices), len(_node_indices)], dtype=int)\n",
        "              for part, direction in zip(partition, directions):\n",
        "\n",
        "                  nonpart = list(_node_indices - set(part))\n",
        "\n",
        "\n",
        "\n",
        "                  if direction == Direction.CAUSE:\n",
        "                      source, target = nonpart, part\n",
        "                  else:\n",
        "                      source, target = part, nonpart\n",
        "                  cut_matrix[np.ix_(source, target)] = 1\n",
        "                  if direction == Direction.BIDIRECTIONAL:\n",
        "                      cut_matrix[np.ix_(target, source)] = 1\n",
        "\n",
        "\n",
        "\n",
        "                    ### Yield all the cuts\n",
        "              yield GeneralSetPartition(\n",
        "                  node_indices,\n",
        "                  cut_matrix,\n",
        "                  node_labels=node_labels,\n",
        "                  set_partition=partition,\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "              ### Yield the cut ignored by the if (both directions in set)\n",
        "  yield GeneralSetPartition(\n",
        "                node_indices,\n",
        "                np.ones((len(_node_indices), len(_node_indices))) - np.identity(len(_node_indices)),\n",
        "                node_labels=node_labels,\n",
        "                set_partition=partition,\n",
        "            )\n",
        "\n",
        "\n",
        "def generate_restricted_bipartitions(node_indices, node_labels=None):\n",
        "    directions = [True, False]\n",
        "    \"\"\"Generate all unidirectional set partitions of a set of nodes.\"\"\"\n",
        "\n",
        "    # Trivial Cut\n",
        "    if len(node_indices) == 1 or config.SYSTEM_PARTITION_INCLUDE_COMPLETE:\n",
        "        yield CompleteGeneralSetPartition(node_indices, node_labels=node_labels)\n",
        "\n",
        "\n",
        "    _node_indices = set(range(len(node_indices)))\n",
        "    for partition in get_bipartitions(_node_indices):\n",
        "        for direction_seq in product(directions, repeat=len(partition)):\n",
        "\n",
        "            # Fill out cut matrix\n",
        "            cut_matrix = np.zeros([len(_node_indices), len(_node_indices)], dtype=int)\n",
        "            for part, direction in zip(partition, direction_seq):\n",
        "                nonpart = list(_node_indices - set(part))\n",
        "                if direction:\n",
        "                    source, target = nonpart, part\n",
        "                else:\n",
        "                    source, target = part, nonpart\n",
        "                cut_matrix[np.ix_(source, target)] = 1\n",
        "\n",
        "\n",
        "                # yield partition\n",
        "            yield GeneralSetPartition(\n",
        "                node_indices,\n",
        "                cut_matrix,\n",
        "                node_labels=node_labels,\n",
        "                set_partition=partition,\n",
        "            )\n",
        "\n",
        "\n",
        "# Only do unique ones- ignores the very last one\n",
        "@system_partition_types.register(\"UNIDIRECTIONAL_BIPARTITIONS\")\n",
        "@functools.wraps(generate_restricted_bipartitions)\n",
        "def unidirectional_set_bipartitions(node_indices, node_labels=None):\n",
        "    # TODO(4.0) generate properly without using set\n",
        "    yield from unique(\n",
        "        generate_restricted_bipartitions(node_indices, node_labels=node_labels)\n",
        "    )\n",
        "\n",
        "\n",
        "def get_phi_distribution(network, state):\n",
        "  subsystem = pyphi.Subsystem(network,state)\n",
        "  distribution = []\n",
        "\n",
        "  def _has_no_cause_or_effect(system_state):\n",
        "    reasons = []\n",
        "    for direction, reason in zip(\n",
        "        Direction.both(),\n",
        "        [ShortCircuitConditions.NO_CAUSE, ShortCircuitConditions.NO_EFFECT],\n",
        "    ):\n",
        "        if system_state[direction].intrinsic_information <= 0:\n",
        "            reasons.append(reason)\n",
        "    return reasons\n",
        "  def sia_return_all_cuts(\n",
        "    subsystem: Subsystem,\n",
        "    repertoire_distance: Optional[str] = None,\n",
        "    directions: Optional[Iterable[Direction]] = None,\n",
        "    partition_scheme: Optional[str] = None,\n",
        "    partitions: Optional[Iterable] = None,\n",
        "    system_state: Optional[SystemStateSpecification] = None,\n",
        "    **kwargs,\n",
        "    ) -> SystemIrreducibilityAnalysis:\n",
        "    \"\"\"Find the minimum information partition of a system.\"\"\"\n",
        "    partition_scheme = fallback(partition_scheme, config.SYSTEM_PARTITION_TYPE)\n",
        "\n",
        "    # TODO(4.0): trivial reducibility\n",
        "\n",
        "    if partitions is None:\n",
        "        filter_func = None\n",
        "        if partitions == \"GENERAL\":\n",
        "\n",
        "            def is_disconnecting_partition(partition):\n",
        "                # Special case for length 1 subsystems so complete partition is included\n",
        "                return (\n",
        "                    not connectivity.is_strong(subsystem.apply_cut(partition).proper_cm)\n",
        "                ) or len(subsystem) == 1\n",
        "\n",
        "            filter_func = is_disconnecting_partition\n",
        "\n",
        "        partitions = system_partitions(\n",
        "            subsystem.node_indices,\n",
        "            node_labels=subsystem.node_labels,\n",
        "            partition_scheme=partition_scheme,\n",
        "            filter_func=filter_func,\n",
        "        )\n",
        "\n",
        "    if system_state is None:\n",
        "        system_state = system_intrinsic_information(\n",
        "            subsystem, directions=directions\n",
        "        )\n",
        "\n",
        "    def _null_sia(**kwargs):\n",
        "        return NullSystemIrreducibilityAnalysis(\n",
        "            system_state=system_state,\n",
        "            node_indices=subsystem.node_indices,\n",
        "            node_labels=subsystem.node_labels,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    if config.SHORTCIRCUIT_SIA:\n",
        "        shortcircuit_reasons = _has_no_cause_or_effect(system_state)\n",
        "        if shortcircuit_reasons:\n",
        "            return _null_sia(reasons=shortcircuit_reasons)\n",
        "\n",
        "    default_sia = _null_sia(reasons=[ShortCircuitConditions.NO_VALID_PARTITIONS])\n",
        "\n",
        "    parallel_kwargs = conf.parallel_kwargs(config.PARALLEL_CUT_EVALUATION, **kwargs)\n",
        "    sias = MapReduce(\n",
        "        evaluate_partition,\n",
        "        partitions,\n",
        "        map_kwargs=dict(\n",
        "            subsystem=subsystem,\n",
        "            system_state=system_state,\n",
        "            repertoire_distance=repertoire_distance,\n",
        "            directions=directions,\n",
        "        ),\n",
        "        shortcircuit_func=utils.is_falsy,\n",
        "        desc=\"Evaluating partitions\",\n",
        "        **parallel_kwargs,\n",
        "    ).run()\n",
        "\n",
        "    # Find MIP in one pass, keeping track of ties\n",
        "    # TODO(ties) refactor into resolve_ties module\n",
        "    mip_sia = default_sia\n",
        "    mip_key = (float(\"inf\"), float(\"-inf\"))\n",
        "    ties = [default_sia]\n",
        "    for sia__ in sias:\n",
        "        distribution.append(sia__.phi)\n",
        "    if phi_dist :\n",
        "      return distribution\n",
        "    for candidate_mip_sia in sias:\n",
        "        candidate_key = sia_minimization_key(candidate_mip_sia)\n",
        "        if candidate_key < mip_key:\n",
        "            mip_sia = candidate_mip_sia\n",
        "            mip_key = candidate_key\n",
        "            ties = [mip_sia]\n",
        "        elif candidate_key == mip_key:\n",
        "            ties.append(candidate_mip_sia)\n",
        "    for tied_mip in ties:\n",
        "        tied_mip.set_ties(ties)\n",
        "    return mip_sia\n",
        "  pyphi.new_big_phi.sia = sia_return_all_cuts\n",
        "  subsystem.sia()\n",
        "  return distribution\n",
        "\n",
        "\n",
        "#### NORMALIZED PHI DISTRIBUTION FUNCTION #####\n",
        "\n",
        "def get_normalized_phi_distribution(network, state):\n",
        "  subsystem = pyphi.Subsystem(network,state)\n",
        "  distribution = []\n",
        "\n",
        "  def _has_no_cause_or_effect(system_state):\n",
        "    reasons = []\n",
        "    for direction, reason in zip(\n",
        "        Direction.both(),\n",
        "        [ShortCircuitConditions.NO_CAUSE, ShortCircuitConditions.NO_EFFECT],\n",
        "    ):\n",
        "        if system_state[direction].intrinsic_information <= 0:\n",
        "            reasons.append(reason)\n",
        "    return reasons\n",
        "\n",
        "  def sia_return_all_cuts(\n",
        "    subsystem: Subsystem,\n",
        "    repertoire_distance: Optional[str] = None,\n",
        "    directions: Optional[Iterable[Direction]] = None,\n",
        "    partition_scheme: Optional[str] = None,\n",
        "    partitions: Optional[Iterable] = None,\n",
        "    system_state: Optional[SystemStateSpecification] = None,\n",
        "    **kwargs,\n",
        "    ) -> SystemIrreducibilityAnalysis:\n",
        "    \"\"\"Find the minimum information partition of a system.\"\"\"\n",
        "    partition_scheme = fallback(partition_scheme, config.SYSTEM_PARTITION_TYPE)\n",
        "\n",
        "    # TODO(4.0): trivial reducibility\n",
        "\n",
        "    if partitions is None:\n",
        "        filter_func = None\n",
        "        if partitions == \"GENERAL\":\n",
        "\n",
        "            def is_disconnecting_partition(partition):\n",
        "                # Special case for length 1 subsystems so complete partition is included\n",
        "                return (\n",
        "                    not connectivity.is_strong(subsystem.apply_cut(partition).proper_cm)\n",
        "                ) or len(subsystem) == 1\n",
        "\n",
        "            filter_func = is_disconnecting_partition\n",
        "\n",
        "        partitions = system_partitions(\n",
        "            subsystem.node_indices,\n",
        "            node_labels=subsystem.node_labels,\n",
        "            partition_scheme=partition_scheme,\n",
        "            filter_func=filter_func,\n",
        "        )\n",
        "\n",
        "    if system_state is None:\n",
        "        system_state = system_intrinsic_information(\n",
        "            subsystem, directions=directions\n",
        "        )\n",
        "\n",
        "    def _null_sia(**kwargs):\n",
        "        return NullSystemIrreducibilityAnalysis(\n",
        "            system_state=system_state,\n",
        "            node_indices=subsystem.node_indices,\n",
        "            node_labels=subsystem.node_labels,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    if config.SHORTCIRCUIT_SIA:\n",
        "        shortcircuit_reasons = _has_no_cause_or_effect(system_state)\n",
        "        if shortcircuit_reasons:\n",
        "            return _null_sia(reasons=shortcircuit_reasons)\n",
        "\n",
        "    default_sia = _null_sia(reasons=[ShortCircuitConditions.NO_VALID_PARTITIONS])\n",
        "\n",
        "    parallel_kwargs = conf.parallel_kwargs(config.PARALLEL_CUT_EVALUATION, **kwargs)\n",
        "    sias = MapReduce(\n",
        "        evaluate_partition,\n",
        "        partitions,\n",
        "        map_kwargs=dict(\n",
        "            subsystem=subsystem,\n",
        "            system_state=system_state,\n",
        "            repertoire_distance=repertoire_distance,\n",
        "            directions=directions,\n",
        "        ),\n",
        "        shortcircuit_func=utils.is_falsy,\n",
        "        desc=\"Evaluating partitions\",\n",
        "        **parallel_kwargs,\n",
        "    ).run()\n",
        "\n",
        "    # Find MIP in one pass, keeping track of ties\n",
        "    # TODO(ties) refactor into resolve_ties module\n",
        "    mip_sia = default_sia\n",
        "    mip_key = (float(\"inf\"), float(\"-inf\"))\n",
        "    ties = [default_sia]\n",
        "    for sia__ in sias:\n",
        "        distribution.append(sia__.normalized_phi)\n",
        "    if phi_dist :\n",
        "      return distribution\n",
        "    for candidate_mip_sia in sias:\n",
        "        candidate_key = sia_minimization_key(candidate_mip_sia)\n",
        "        if candidate_key < mip_key:\n",
        "            mip_sia = candidate_mip_sia\n",
        "            mip_key = candidate_key\n",
        "            ties = [mip_sia]\n",
        "        elif candidate_key == mip_key:\n",
        "            ties.append(candidate_mip_sia)\n",
        "    for tied_mip in ties:\n",
        "        tied_mip.set_ties(ties)\n",
        "    return mip_sia\n",
        "  pyphi.new_big_phi.sia = sia_return_all_cuts\n",
        "  subsystem.sia()\n",
        "  return distribution\n",
        "\n",
        "def generate_restricted_partitions(node_indices, node_labels=None):\n",
        "    \"\"\"Generate all unidirectional set partitions of a set of nodes.\"\"\"\n",
        "\n",
        "\n",
        "    ### Trivial Cut\n",
        "    if len(node_indices) == 1 or config.SYSTEM_PARTITION_INCLUDE_COMPLETE:\n",
        "        yield CompleteGeneralSetPartition(node_indices, node_labels=node_labels)\n",
        "\n",
        "\n",
        "    directions = [True, False]\n",
        "    _node_indices = set(range(len(node_indices)))\n",
        "    for partition in combinatorics.set_partitions(_node_indices, nontrivial=True):\n",
        "        for direction_seq in product(directions, repeat=len(partition)):\n",
        "\n",
        "\n",
        "            ### Fill out cut matrix\n",
        "            cut_matrix = np.zeros([len(_node_indices), len(_node_indices)], dtype=int)\n",
        "            for part, direction in zip(partition, direction_seq):\n",
        "                nonpart = list(_node_indices - set(part))\n",
        "                if direction:\n",
        "                    source, target = nonpart, part\n",
        "                else:\n",
        "                    source, target = part, nonpart\n",
        "                cut_matrix[np.ix_(source, target)] = 1\n",
        "\n",
        "\n",
        "                ### Yield the cuts\n",
        "            yield GeneralSetPartition(\n",
        "                node_indices,\n",
        "                cut_matrix,\n",
        "                node_labels=node_labels,\n",
        "                set_partition=partition,\n",
        "            )\n",
        "\n",
        "\n",
        "### Yield the unique partitions only\n",
        "@system_partition_types.register(\"UNIDIRECTIONAL_PARTITIONS\")\n",
        "@functools.wraps(generate_restricted_partitions)\n",
        "def unidirectional_set_partitions(node_indices, node_labels=None):\n",
        "    # TODO(4.0) generate properly without using set\n",
        "    yield from unique(\n",
        "        generate_restricted_partitions(node_indices, node_labels=node_labels)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "### Generate All Possible Bi-Partitions Of A Set Of Nodes\n",
        "def get_bipartitions(collection):\n",
        "    collection = list(collection)\n",
        "    n = len(collection)\n",
        "    # Special case: no bipartitions possible if the collection has fewer than 2 elements\n",
        "    if n < 2:\n",
        "        return\n",
        "    # Generate all combinations for the first subset of size from 1 to n-1\n",
        "    for i in range(1, n):\n",
        "        for first_subset in combinations(collection, i):\n",
        "            second_subset = [item for item in collection if item not in first_subset]\n",
        "            yield [list(first_subset), second_subset]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_noisy_network(size):\n",
        "    ### Obtains a random network of nodes of a given size, directly ready for computing Phi.\n",
        "    def normalize_rows(matrix):\n",
        "        ### Scales probabilities in a matrix so that they satisfy conditional independence\n",
        "        num_rows = matrix.shape[0]\n",
        "        normalized_matrix = np.zeros_like(matrix)# Create an empty matrix of the same shape\n",
        "        for i in range(num_rows):\n",
        "            row = matrix[i, :]\n",
        "            current_sum = np.sum(row)\n",
        "            if current_sum > 0:\n",
        "                scaling_factor = 1.0 / current_sum\n",
        "                normalized_row = row * scaling_factor\n",
        "                normalized_matrix[i, :] = normalized_row\n",
        "            else: print(\"Zero_Sum_Error: Problem With Values Generated\")\n",
        "        return normalized_matrix\n",
        "\n",
        "\n",
        "    return pyphi.Network(normalize_rows(pyphi.convert.state_by_node2state_by_state(np.random.rand(2**size,size))))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = 4\n",
        "sample_size = 40\n",
        "True_phi = []\n",
        "Sample_phi = []\n",
        "Unbiased_Phi = []\n",
        "trial = 0\n",
        "while trial <  100:\n",
        "\n",
        "  ### Generate Network\n",
        "  network = create_noisy_network(size)\n",
        "  state = [np.random.choice([0,1]) for i in range(size)]\n",
        "  network = create_noisy_network(size)\n",
        "  subsystem = pyphi.Subsystem(network,state)\n",
        "\n",
        "\n",
        "\n",
        "  ### Gather Normalized Phi Values for Each Cut\n",
        "  phi_dist =  True\n",
        "  normalized_phi_vals = get_normalized_phi_distribution(network, state)\n",
        "  n = len(normalized_phi_vals)\n",
        "  true_nphi= np.min(normalized_phi_vals)\n",
        "  #print(\"True Normalized Phi:\",true_nphi)\n",
        "\n",
        "  if true_nphi < 0:\n",
        "    continue\n",
        "\n",
        "  ### Break Ties In The Distribution:\n",
        "  normalized_phi_vals = normalized_phi_vals + np.random.normal(loc = 0, scale = 0.000001, size=len(normalized_phi_vals))\n",
        "\n",
        "  ### Collect My Sample Data\n",
        "  sample = np.random.choice(normalized_phi_vals, sample_size, replace = False)\n",
        "  sample_minimum_nphi = np.min(sample)\n",
        "  #print(\"Sample Normalized Phi:\",sample_minimum_nphi)\n",
        "\n",
        "\n",
        "\n",
        "  ### Obtain Unbiased Estimate For a\n",
        "  truncated_sample = sample[sample > sample_minimum_nphi]\n",
        "  def ordered_adjusted_sum(array):\n",
        "    N = len(array)\n",
        "    sorted_array = np.sort(array)\n",
        "    s = 0\n",
        "    for i in range(2, N + 1):\n",
        "      s = s + (sorted_array[i-2])*( (N**(-N))*((N - i + 1)**N - (N-i)**N ))\n",
        "    return s\n",
        "  a_unbiased = (sample_minimum_nphi - ordered_adjusted_sum(truncated_sample))/ (1 - ((sample_size-1)/sample_size)**sample_size)\n",
        "  #print(\"Unbiased Estimate For a:\", a_unbiased)\n",
        "\n",
        "\n",
        "\n",
        "  if true_nphi > 0:\n",
        "    trial += 1\n",
        "    True_phi.append(true_nphi)\n",
        "    Sample_phi.append(sample_minimum_nphi)\n",
        "    Unbiased_Phi.append(a_unbiased)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Mean Error: a_unbiased - a_true:\",  np.mean(np.array(Unbiased_Phi) - np.array(True_phi)))\n",
        "print(\"Mean Error: a_sample - a_true:\",  np.mean(np.array(Sample_phi) - np.array(True_phi)))\n",
        "\n",
        "print(\"Mean Absolute Error: a_unb - a_true:\", np.mean(np.abs(np.array(Unbiased_Phi) - np.array(True_phi))))\n",
        "print(\"Mean Absolute Error: a_sample - a_true:\", np.mean(np.abs(np.array(Sample_phi) - np.array(True_phi))))\n",
        "\n",
        "print(\"Mean Square Error: a_unb - a_true:\", np.mean(np.square(np.array(Unbiased_Phi) - np.array(True_phi))))\n",
        "print(\"Mean Square Error: a_sample - a_true:\", np.mean(np.square(np.array(Sample_phi) - np.array(True_phi))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UncUY1wu5ORk",
        "outputId": "35146583-0229-4c33-88a1-1d475b8243e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Error: a_unbiased - a_true: 0.004575808556065232\n",
            "Mean Error: a_sample - a_true: 0.01110137854781399\n",
            "Mean Absolute Error: a_unb - a_true: 0.011964352749986895\n",
            "Mean Absolute Error: a_sample - a_true: 0.01110137856965897\n",
            "Mean Square Error: a_unb - a_true: 0.0002563203348687891\n",
            "Mean Square Error: a_sample - a_true: 0.0002813418191439747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = 4\n",
        "sample_size = 40\n",
        "True_phi = []\n",
        "Sample_phi = []\n",
        "Unbiased_Phi = []\n",
        "trial = 0\n",
        "while trial <  100:\n",
        "\n",
        "  ### Generate Network\n",
        "  network = create_noisy_network(size)\n",
        "  state = [np.random.choice([0,1]) for i in range(size)]\n",
        "  network = create_noisy_network(size)\n",
        "  subsystem = pyphi.Subsystem(network,state)\n",
        "\n",
        "\n",
        "\n",
        "  ### Gather Normalized Phi Values for Each Cut\n",
        "  phi_dist =  True\n",
        "  normalized_phi_vals = get_normalized_phi_distribution(network, state)\n",
        "  n = len(normalized_phi_vals)\n",
        "  true_nphi= np.min(normalized_phi_vals)\n",
        "  #print(\"True Normalized Phi:\",true_nphi)\n",
        "\n",
        "  if true_nphi < 0:\n",
        "    continue\n",
        "\n",
        "  ### Break Ties In The Distribution:\n",
        "  normalized_phi_vals = normalized_phi_vals + np.random.normal(loc = 0, scale = 0.000001, size=len(normalized_phi_vals))\n",
        "\n",
        "  ### Collect My Sample Data\n",
        "  sample = np.random.choice(normalized_phi_vals, sample_size, replace = False)\n",
        "  sample_minimum_nphi = np.min(sample)\n",
        "  #print(\"Sample Normalized Phi:\",sample_minimum_nphi)\n",
        "\n",
        "\n",
        "\n",
        "  ### Obtain Unbiased Estimate For a\n",
        "  truncated_sample = sample[sample > sample_minimum_nphi]\n",
        "  def ordered_adjusted_sum(array):\n",
        "    N = len(array)\n",
        "    sorted_array = np.sort(array)\n",
        "    s = 0\n",
        "    for i in range(2, N + 1):\n",
        "      s = s + (sorted_array[i-2])*( (N**(-N))*((N - i + 1)**N - (N-i)**N ))\n",
        "    return s\n",
        "  a_unbiased = (sample_minimum_nphi - ordered_adjusted_sum(truncated_sample))/ (1 - ((sample_size-1)/sample_size)**sample_size)\n",
        "  #print(\"Unbiased Estimate For a:\", a_unbiased)\n",
        "\n",
        "\n",
        "\n",
        "  if true_nphi > 0:\n",
        "    trial += 1\n",
        "    True_phi.append(true_nphi)\n",
        "    Sample_phi.append(sample_minimum_nphi)\n",
        "    Unbiased_Phi.append(a_unbiased)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Mean Error: a_unbiased - a_true:\",  np.mean(np.array(Unbiased_Phi) - np.array(True_phi)))\n",
        "print(\"Mean Error: a_sample - a_true:\",  np.mean(np.array(Sample_phi) - np.array(True_phi)))\n",
        "\n",
        "print(\"Mean Absolute Error: a_unb - a_true:\", np.mean(np.abs(np.array(Unbiased_Phi) - np.array(True_phi))))\n",
        "print(\"Mean Absolute Error: a_sample - a_true:\", np.mean(np.abs(np.array(Sample_phi) - np.array(True_phi))))\n",
        "\n",
        "print(\"Mean Square Error: a_unb - a_true:\", np.mean(np.square(np.array(Unbiased_Phi) - np.array(True_phi))))\n",
        "print(\"Mean Square Error: a_sample - a_true:\", np.mean(np.square(np.array(Sample_phi) - np.array(True_phi))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt0JG8EQa1JE",
        "outputId": "f30c4ce7-99d3-4ccc-be02-f54b1e1098b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Error: a_unbiased - a_true: 0.006539676231835703\n",
            "Mean Error: a_sample - a_true: 0.014494530592427246\n",
            "Mean Absolute Error: a_unb - a_true: 0.016058644899669888\n",
            "Mean Absolute Error: a_sample - a_true: 0.014494530592427246\n",
            "Mean Square Error: a_unb - a_true: 0.00034089009872981176\n",
            "Mean Square Error: a_sample - a_true: 0.00037344906767557454\n"
          ]
        }
      ]
    }
  ]
}